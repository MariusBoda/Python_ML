{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d560f6a3e2c67bb150536f7e7d89562",
     "grade": false,
     "grade_id": "cell-0ef3a793cfc0e37b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<h2 align=\"center\">Regression</h2>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center>CS-EJ3211 Machine Learning with Python 29.05.-17.07.2023</center>\n",
    "<center>Aalto University (Espoo, Finland)</center>\n",
    "<center>fitech.io (Finland)</center>\n",
    "\n",
    "\n",
    "<img src=\"../../../coursedata/2_Regression/facades.jpg\" width=500/>\n",
    "\n",
    "This notebook considers machine learning (ML) methods that use the features of a data point to predict the value of a **numeric** label $y \\in \\mathbb{R}$. ML applications involving numeric label values are often referred to as **regression problems**. You will apply some basic **regression methods** to predict the median apartment value of a given neighborhood in the Helsinki area. \n",
    "\n",
    "**Regression methods** learn a useful hypothesis map or predictor function $h(\\mathbf{x})$ by minimizing the average loss incurred on some labeled datapoints (the training set). The function value $h(\\mathbf{x})$ is used as a prediction (estimate or guess) for the numeric label $y$ of a data point based on the features $\\mathbf{x}$. What sets regression methods apart from **classification methods** is mainly the choice for the loss function. As a point in case, there are regression and classification methods that use the same hypothesis space of prediction functions (see linear regression and logistic regression) but differ in the loss function used to assess the quality of a predictor function. \n",
    "\n",
    "The various regression methods differ in their choice of allowed predictor functions (hypothesis space) and loss function (the quality measure used to rank predictors). These different combinations offer different tradeoffs between **computational complexity, robustness (against outliers), and prediction error**. A widely-used regression method is linear regression which uses linear predictor maps (linear hypothesis space) and the squared error loss. This combination is computationally appealing as the resulting method can be implemented using efficient gradient-based optimization algorithms (see [Ch. 5, MLBook](https://github.com/alexjungaalto/MachineLearningTheBasics/blob/master/MLBasicsBook.pdf)). However, using the squared error loss makes the method sensitive to outliers in the data. We will then see how another regression method that uses a different loss function is more robust against outliers. This improved statistical robustness comes at the cost of increased computational complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab05ef26a3608bd6b1d9128ca94092d3",
     "grade": false,
     "grade_id": "cell-4872a33f422acba9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Learning Goals\n",
    "\n",
    "After completing this notebook, you should\n",
    "\n",
    "- be able to formulate applications as regression problems by identifying data points, their features, and labels. \n",
    "- be able to represent features and labels of data points as `NumPy arrays`.\n",
    "- be able to use the methods for linear regression provided by the Python package `scikit-learn`.\n",
    "- know about the effect of varying number of features on the training error. \n",
    "- know about the effect of varying size of training set on the training error. \n",
    "- know different statistical and computational properties of squared error loss and the Huber loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f455260e527af8f046366bcf8ce738c",
     "grade": false,
     "grade_id": "cell-200ca65238fcf8cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Reading Material \n",
    "\n",
    "* Chapter 3.1, 3.3 of [course book](https://github.com/alexjungaalto/MachineLearningTheBasics/blob/master/MLBasicsBook.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d8b5f030b19e8b015d0c033ba413568",
     "grade": false,
     "grade_id": "cell-d64aa667351e92ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## A Real-Estate Regression Problem \n",
    "\n",
    "Assume you have secured an internship at the Helsinki office of a real estate investment fund. You have been placed in the research team, and your first task is to develop a ML method for predicting the median house price for different Helsinki neighborhoods. Some of your colleagues have collected a dataset $\\mathcal{D}$ containing information about $20$ different neighborhoods. \n",
    "\n",
    "We consider each neighborhood as an invididual datapoint. Each datapoint (neighborhood) is characterized by several features such as the average number of rooms in the apartments, the percentage of buildings constructed before 1970 in the neighborhood, as well as eight experimental features designed by your colleagues. The quantity of interest or label for each datapoint is the median house price for a neighborhood. Some real-estate experts have determined the median apartment price for each neighborhood in the dataset $\\mathcal{D}$. The salary for a real-estate expert costs a fortune. Therefore, we want to use ML to learn a predictor function $h(\\mathbf{x})$ that is able to predict the median apartment price $y$ from the features $\\mathbf{x}$ of that neighborhood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c914fbc1393a444a16de3f3826e3652",
     "grade": false,
     "grade_id": "cell-1a60d1bfa6cdee14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Data\n",
    "\n",
    "Let us summarize the above ML problem: datapoints represent neighborhoods that are characterized by $n$ different features that we stack into a feature vector $\\mathbf{x}=\\big(x_{1},\\ldots,x_{n}\\big)^{T}$. The features are properties of a neighborhood that can be determined easily in an automated fashion. Each datapoint (neighborhood) has a numeric label $y$ which is the (true) median apartment price of the neighborhood. \n",
    "\n",
    "Finding out the label for a datapoint requires real-estate experts who are costly. This notebook studies ML methods that use the labeled datapoints in our dataset $\\mathcal{D}$ to learn a predictor $h(\\mathbf{x})$. This predictor function is used to predict the median apartment value of an arbitrary neighborhood (also outside our dataset $\\mathcal{D}$) by using the features $\\mathbf{x}$ of that neighborhood. \n",
    "\n",
    "To learn a predictor $h(\\mathbf{x})$ we use our dataset $\\mathcal{D}$ consisting of $m=20$ neighborhoods for which the label values have been determined. In ML terms, our dataset is a labelled dataset (since we know the label values) $$\\mathcal{D} = \\big(\\mathbf{x}^{(1)}, y^{(1)}\\big), \\big(\\mathbf{x}^{(2)}, y^{(2)}\\big), \\ldots, \\big(\\mathbf{x}^{(m)}, y^{(m)}\\big).$$ \n",
    "Here, $\\mathbf{x}^{(i)} = \\big(x_1^{(i)}, x_2^{(i)}, \\ldots, x_n^{(i)} \\big)^T$ and $y^{(i)}$ denote the feature vector and label, respectively, of the $i$-th data point (neighborhood). It will be convenient to collect the feature vectors of our dataset $\\mathcal{D}$ in the rows of the **feature matrix** $\\mathbf{X}$,\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbf{X} = \\begin{pmatrix}\n",
    "                  \\mathbf{x}^{(1)} & \\mathbf{x}^{(2)} & \\ldots & \\mathbf{x}^{(m)}\n",
    "               \\end{pmatrix}^T \n",
    "             = \\begin{pmatrix}\n",
    "                  x_1^{(1)} & x_2^{(1)} & \\ldots & x_n^{(1)} \\\\\n",
    "                  x_1^{(2)} & x_2^{(2)} & \\ldots & x_n^{(2)} \\\\\n",
    "                  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                  x_1^{(m)} & x_2^{(m)} & \\ldots & x_n^{(m)}\n",
    "               \\end{pmatrix} \\in \\mathbb{R}^{m \\times n}\n",
    "\\end{equation}\n",
    "\n",
    "The $i$-th row of the feature matrix $\\mathbf{X}$ contains the feature vector of the $i$-th data point. \n",
    "Furthermore, it will be convenient to collect the labels of our dataset $\\mathcal{D}$ in a **label vector** $\\mathbf{y}$, \n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbf{y} = \\begin{pmatrix}\n",
    "                  {y}^{(1)} \\\\ {y}^{(2)} \\\\ \\vdots \\\\ {y}^{(m)} \n",
    "               \\end{pmatrix} \\in \\mathbb{R}^m\n",
    "\\end{equation}\n",
    "The $i$-th entry of the label vector is the label of the $i$:th data point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2f949dd1e034da6d605c2b972173aa4",
     "grade": false,
     "grade_id": "cell-85c59fa9b7d67b83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The Python package [`NumPy`](https://numpy.org/) provides methods for manipulating numeric arrays which are stored as [`ndarray`](https://numpy.org/doc/stable/reference/arrays.ndarray.html) objects. These objects are used to represent mathematical objects such as vectors or matrices. In particular, we can  store a feature matrix, whose rows are the feature vectors of datapoints, as a 2-dimensional `ndarray` of shape `(m,n)`. That is, it has `m` rows (data points) and `n` columns (features). This `ndarray` format for matrices is also used by the ML methods provided by the Python package [`scikit-learn`](https://scikit-learn.org/stable/). \n",
    "\n",
    "If we were to apply the mathematical formulation fully in NumPy, we would define the label vector as a 2-dimensional `ndarray` of shape `(m,1)` that corresponds to a column vector. However, most methods in Scikit-learn operate under the assumption that the label vector is a 1-dimensional `ndarray` of shape `(m,)`, so we adopt this practice instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "261b6e670c7b0a4ef20a263d8d7bbb4a",
     "grade": false,
     "grade_id": "cell-af1923f46c54cd28",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='handsondata'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "    \n",
    "### Demo. Load the data.\n",
    "    \n",
    "In the code snippet below, we create a function for loading the feature matrix `X` and label vector `y` for the apartment value problem. The function returns the feature matrix `X` of shape `(m,n)` and the label vector `y` of shape `(m,)`.\n",
    "    \n",
    "In most cases, it is not necessary to create a separate function for loading the data if the procedure is this simple. However, it is convenient in this case as we will reload the data before many tasks to make sure that the dataset has not been modified. \n",
    "    \n",
    "At the end of the cell, we use the newly defined function to load the data and print the shape of the feature matrix and label vector.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e8239ce785cce1bf9ceece4a27ded88",
     "grade": false,
     "grade_id": "cell-a98cfb98188a9730",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10) (20,)\n"
     ]
    }
   ],
   "source": [
    "# Import basic libraries needed\n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def load_housing_data(m=20, n=10):\n",
    "    # Load dataframe from csv\n",
    "    df = pd.read_csv(\"../../../coursedata/2_Regression/helsinki_apartment_prices.csv\", index_col=0)  \n",
    "    \n",
    "    # Extract feature matrix and label vector from dataframe\n",
    "    X = df.iloc[:m,:n].to_numpy()\n",
    "    y = df.iloc[:m,-1].to_numpy().reshape(-1)\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "# Load the housing data\n",
    "X, y = load_housing_data()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "249e94734fd8671cc2db97c6bc028933",
     "grade": false,
     "grade_id": "cell-b85dc13f39e40ddd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Visualize Data\n",
    "\n",
    "Scatter plots visualize data points by representing them as \"dots\" in the two-dimensional plane. Scatter plots can help to develop an intuition for the relation between features and labels of data points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a931f2467dbd73d10f5daa48b3ce3d46",
     "grade": false,
     "grade_id": "cell-9fa6ce321f69617f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='drawplot'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "### Demo. Scatterplots of Features and Labels\n",
    "    \n",
    "The code below generates scatterplots of the the real-estate dataset. We generate a separate scatterplot for each of the first three features $x_{1},x_{2},x_{3}$ of a datapoint. Each scatterplot depicts the datapoints by dots whose coorindates are given by the label $y$, representing the median value of apartments, and one of the first three features $x_1, x_2, x_3$ of a datapoint. Scatterplots allow for an analysis of possible relationships between the features and the label of datapoints. Generating and inspecting scatterplots of data is a very useful first step when starting on a ML problem.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00998fef5ad295650b20430051c4252a",
     "grade": false,
     "grade_id": "cell-9e4c40988deb13df",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure with 3 subplots in 1 row \n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,5))\n",
    "\n",
    "# Create first subplot \n",
    "ax[0].scatter(X[:,0], y)\n",
    "ax[0].set_title('first feature $x_{1}$ vs. label $y$')\n",
    "ax[0].set_xlabel('$x_{1}$: Average number of rooms')\n",
    "ax[0].set_ylabel('$y$: Median apartment value (10000€)')\n",
    "\n",
    "# Create second subplot \n",
    "ax[1].scatter(X[:,1], y)\n",
    "ax[1].set_xlabel('$x_{2}$: Proportion of buildings built <1970')\n",
    "ax[1].set_title('second feature $x_{2}$ vs. label $y$')\n",
    "\n",
    "# Create second subplot \n",
    "ax[2].scatter(X[:,2], y)\n",
    "ax[2].set_xlabel('$x_{3}$: Experimental feature #1')\n",
    "ax[2].set_title('third feature $x_{3}$ vs. label $y$')\n",
    "\n",
    "# Display the figure containing two subplots \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61507654f75383eb4dd6c2362147c720",
     "grade": false,
     "grade_id": "cell-26aa2ceb66c02fb5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By looking at the scatterplots above, we can see that the median apartment value seems to be related to at least some of the features. As one would expect, the average number of rooms seems to be positively correlated with the median value. In contrast, the first experimental feature $x_3$ does not seem very informative. We can only hope that the research team has done a better job with the rest of the experimental features.\\\n",
    "Scatter plots can also be handy for detecting strongly correlated features, e.g., by plotting ${x}_{1}$ vs ${x}_{2}$, and so on. This might be an important step in the modelling process, as some models are harder to train and interpret in the presence of strongly correlated features. However, visual inspection becomes impractical as the number of features grows large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7870bde3aa2cf9002bf51feff3962b53",
     "grade": false,
     "grade_id": "cell-ba1d8078dc67137d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Linear Regression \n",
    "\n",
    "As discussed in the first notebook, given limited computational resources, ML methods cannot search for the best predictor function $h(\\mathbf{x})$ over the entire space of all possible functions that map the feature vector $\\mathbf{x}$ to a predicted label $\\hat{y}$. This set of functions is simply too large. \n",
    "\n",
    "Every practical ML method uses a small subset of candidate predictor functions. This subset of candidate predictor functions is referred to as the **hypothesis space** or **model** underlying the ML method. Chapter 3 of [mlbook.cs.aalto.fi](https://github.com/alexjungaalto/MachineLearningTheBasics/blob/master/MLBasicsBook.pdf) details how some of the most widely-used ML methods are obtained for specific choices for the hypothesis space. \n",
    "\n",
    "One of the most important example of a hypothesis space is the space of linear functions, \n",
    "\\begin{equation*}\n",
    "h^{(w_{0},\\mathbf{w})}(\\mathbf{x}) = w_0 + \\mathbf{w}^{T} \\mathbf{x} = w_0 + \\sum_{i=1}^n w_i x_i. \n",
    "\\tag{Eq1}\n",
    "\\end{equation*}\n",
    "These linear functions are parametrized by the weight vector $\\mathbf{w}=\\big(w_1, w_2, \\ldots, w_n \\big)^T$ and the intercept term $w_0$. Each linear function ${\\rm (Eq1)}$ is fully determined by the values of the weights and intercept. \n",
    "\n",
    "The linear function ${\\rm (Eq1)}$ adds to the intercept term a weighted sum of the features $x_1, x_2, \\ldots, x_n$ using the weights $w_1, w_2, \\ldots, w_n$. Using linear predictor functions can be expected to work well if the features $\\mathbf{x}$ of a data point are (at least approximatley) linearly related to the label $y$. \n",
    "\n",
    "Now that we have defined the form of the functions in the hypothesis space, we want to fit the model by finding the predictor function that results in the best predictions on the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "304523f2de55f1821a07ae2fb97e7f67",
     "grade": false,
     "grade_id": "cell-e69f3dd1553aa6bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training a linear regression model by minimizing the average loss\n",
    "\n",
    "A loss function $\\mathcal{L}(\\hat{y}, y)$ quantifies the quality of a single prediction $\\hat{y}=h(\\mathbf{x})$ by comparing it to the true label $y$ of a data point. The basic idea is that good predictions will only incur a small loss, whereas bad predictions will incur a large loss. \n",
    "\n",
    "Given a set of labelled data and a loss function, we can evaluate the fit of a predictor function on the dataset $\\mathcal{D}$ by computing the **average loss**, \n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:emprisk} \\tag{Eq2}\n",
    "    \\mathcal{E} \\big(h(\\mathbf{x})|\\mathcal{D} \\big) = (1/m) \\sum_{i=1}^m \\mathcal{L}(\\hat{y}^{(i)}, {y}^{(i)}).\n",
    "\\end{equation}\n",
    "\n",
    "A popular choice for the loss function in regression problems is the **squared error loss**\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\hat{y}, y) = (y - \\hat{y})^2.\n",
    "\\end{equation}\n",
    "\n",
    "Inserting the squared error loss into \\eqref{eq:emprisk} results in the **mean squared error** (MSE)\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{E}\\big(h(\\mathbf{x})|\\mathcal{D}\\big) = \\frac{1}{m} \\sum_{i=1}^m ({y}^{(i)} - \\hat{y}^{(i)})^2 = \\frac{1}{m} \\sum_{i=1}^m ({y}^{(i)} - h(\\mathbf{x}^{(i)}))^2 = \\frac{1}{m} \\sum_{i=1}^m ({y}^{(i)} - w_0 - \\mathbf{w}^T\\mathbf{x}^{(i)})^2.\n",
    "\\end{equation}\n",
    "\n",
    "It seems quite natural to learn a predictor function $h \\in \\mathcal{H}$ that minimizes the average loss over the training set, \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{h} = \\mbox{argmin}_{h\\in \\mathcal{H}}  \\mathcal{E} \\big(h(\\mathbf{x})|\\mathcal{D} \\big). \n",
    "\\end{equation}\n",
    "\n",
    "For linear predictor maps $h^{(w_{0},\\mathbf{w})}$, which are parametrized by the weight vector $\\mathbf{w}$ and intercept $w_{0}$, the optimal weight vector is obtained as \n",
    "\\begin{equation}\n",
    "\\hat{w}_{0},\\hat{\\mathbf{w}} = \\mbox{argmin}_{w_{0},\\mathbf{w}}  \\frac{1}{m} \\sum_{i=1}^m ({y}^{(i)} - w_0 - \\mathbf{w}^T\\mathbf{x}^{(i)})^2.\n",
    " \\tag{Eq3}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fffdc3900b4c6024a4088990cba1089",
     "grade": false,
     "grade_id": "cell-5eb2e2fdcba5ac36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='drawplot'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "### Demo. The Linear Regression Class.\n",
    "\n",
    "The code snippet below demonstrates how to use the Python package [`scikit-learn`](https://scikit-learn.org/stable/) to learn a linear predictor for the median appartment price of a neighborhood based on its features. To start with, we will only use the first two features $x_1$ (average number of rooms in apartments) and $x_2$ (percentage of buildings constructed before 1970) of a datapoint to predict its label $y$ (median appartment price). \n",
    "    \n",
    "The class [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) provided by `scikit-learn` allows to learn a linear predictor map that incurs a minimum average squared error loss on given labeled datapoints (the training data). The use of this class is as follows: \n",
    "\n",
    "1. We store the feature vectors (using $n=2$ features for each datapoint) and labels for the datapoints in our real-estate dataset $\\mathcal{D}$ into the numpy arrays `X` and `y`. \n",
    "    \n",
    "2. We create an instance (or object) of the `LinearRegression` class and name it `reg`. This is done with the statement `LinearRegression()`. The constructor of `LinearRegression()` allows to specificy different parameters. For example, the parameter `fit_intercept` determines if an intercept term $w_{0}$ is used or not. When setting `fit_intercept=False`, the intercept term is fixed to $w_{0}=0$. \n",
    "\n",
    "3. After creating the `LinearRegression` object `reg`, we learn the optimal weights and intercept by minimizing the mean squared error incurred when predicting the labels in `y` using the feature vectors in `X`. This is done using the method `reg.fit()`. \n",
    "\n",
    "4. The optimal weights and intercept, computed by the method `reg.fit()`, are stored in the attributes `reg.coef_` and `reg.intercept_`. \n",
    "\n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a151846cc020febcbf13aeea1d8f7cad",
     "grade": false,
     "grade_id": "cell-2508c369e6800130",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from IPython.display import display, Math\n",
    "\n",
    "# Load housing data with two features\n",
    "X, y = load_housing_data(n=2)\n",
    "\n",
    "# Create the linear regression object\n",
    "reg = LinearRegression(fit_intercept=True) \n",
    "\n",
    "# Fit the linear regression model  \n",
    "reg.fit(X, y)\n",
    "\n",
    "# Get the optimal weight vector w of the fitted model \n",
    "w_opt = reg.coef_\n",
    "\n",
    "# Get the optimal intercept of the fitted model  \n",
    "w_intercept = reg.intercept_\n",
    "\n",
    "# Print the optimal weight vector \n",
    "display(Math(r'$\\mathbf{w}_{\\rm opt} ='))\n",
    "print(w_opt)\n",
    "\n",
    "# Print the optimal intercept\n",
    "display(Math(r'${w}_{0 \\rm opt} ='))\n",
    "print(w_intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e091915e07c10a847ba835083f534a5b",
     "grade": false,
     "grade_id": "cell-ba7892d8ce8fd0ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The Training Error\n",
    "\n",
    "The optimization problem ${\\rm (Eq3)}$ is known as empirical risk minimization (ERM). Solving the ERM problem for linear regression provides the optimum weights $\\hat{\\mathbf{w}}$ and optimal intercept $\\hat{w}_{0}$. The average loss incurred by these optimum weights is often referred to as the **training error** of linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00ed2447fdc927f9a785ea6d54a43a46",
     "grade": false,
     "grade_id": "cell-049bdf44b143edcb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='drawplot'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "### Demo. Calculating the (Optimal) Training Error for Linear Regression\n",
    "   \n",
    "After learning the optimal weights for a linear predictor we can now apply this linear predictor to any datapoint with known features. In particular, we can apply the linear predictor to the datapoints used to learning the optimal weights. \n",
    "    \n",
    "1. We use the method `reg.predict(X)` to compute the predictions obtained by applying the linear predictor with weighs `reg.coeff_` and intercept `reg.intercept_` to the features stored in `X`. This function takes as input a feature matrix with `m` rows of data points and returns a vector of predicted labels of shape `(m,)`. The resulting predicted labels are stored in the numpy array `y_pred`. \n",
    " \n",
    "2. After computing the predicted labels `y_pred`, we calculate the training error by comparing the predictions to the true labels `y`. Since the training error is in this case defined as the MSE, we will use the `mean_squared_error(y, y_pred)` function from the `metrics` module in Scikit-learn to calculate it.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "951c506d208c2646c075d6a0ca72421c",
     "grade": false,
     "grade_id": "cell-577688b1df8c48f9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    " \n",
    "# Calculate the predicted labels of the data points in the training set\n",
    "y_pred = reg.predict(X)\n",
    "\n",
    "# Calculate the MSE of the true and predicted labels of the training set\n",
    "training_error = mean_squared_error(y, y_pred)\n",
    "\n",
    "# Print training error \n",
    "print(\"\\nThe resulting mean squared error (training error) is \", training_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cdc05759b2b3cc5fa9a66fc73a8c13f5",
     "grade": false,
     "grade_id": "cell-4c05f70158a6e79a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have now trained our first model on the dataset and obtained a value for the training error. Next, we might want to try out to add some other features to the dataset. By comparing the training errors of the optimal predictor of models trained on different features, we can find the model that fits the training data the best.\n",
    "\n",
    "Your task below is to try out a set of models for median apartment value prediction that utilize a different number of features in the original dataset. In the context of this problem, it is of particular interest to examine if the inclusion of the experimental features developed by the research team results in better predictions on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a9cb20b83002022bd43e149b562e5f1",
     "grade": false,
     "grade_id": "cell-a5ee14879390235c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student Task 2.1. Varying the Number of Features. \n",
    "    \n",
    "This task requires you to train 10 different hypothesis spaces or models. We index these ten models using the number $r=1,\\ldots,10$. The $r$-th model is given by all linear predictor functions that only use first $r$ features of a datapoint. The first model ($r=1$) uses only the feature $x_1$. The second model ($r=2$) uses the features $x_1,x_2$ and so on. The last model ($r=10$) uses ten features $x_1,x_2,\\ldots,x_{10}$. \n",
    "    \n",
    "For each model $r=1,\\ldots,10$, you should\n",
    "    \n",
    "- Create a linear regression model with an intercept term (`fit_intercept=True`) and train the model on the data using only the $r$ first features. For example, if $r=4$, you should train the model using a feature matrix containing only the features $x_1,x_2,x_3,x_4$.\n",
    "    \n",
    "    \n",
    "- Use the trained model to calculate the predicted labels `y_pred` of the dataset used for training the model. Notice that the feature matrix used for predicting labels must have the same number of features as the matrix used for training. \n",
    "    \n",
    "\n",
    "- Calculate the mean squared error of `y_pred` in comparison to `y`, and store this error in the $r$th element of the array `linreg_error`. Remember that indexing starts at 0 in Python, so the $r$th element is found at index $r-1$ in the array.\n",
    "\n",
    "\n",
    "You can use the above demo code snippets to complete this student task. The main challenge is to iterate over the different values of $r$, and to figure out how to modify the feature matrix to only contain the first $r$ features during the $r$th iteration.\n",
    "    \n",
    "**Hint:** Use a for loop to iterate over the number of features $r=1,\\ldots,10$.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f96a40f481bd7a11d2adf40e6a067452",
     "grade": false,
     "grade_id": "cell-adb3039336234c0d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error             \n",
    "\n",
    "max_r = 10\n",
    "\n",
    "# Load the dataset using 10 features \n",
    "X, y = load_housing_data(n=10)\n",
    "  \n",
    "# Vector for storing the training error for each r\n",
    "linreg_error = np.zeros(max_r)    \n",
    "\n",
    "### STUDENT TASK ###\n",
    "# Loop max_r times:\n",
    "#  reg = ...\n",
    "#  reg.xxx(...)\n",
    "#  y_pred = ...\n",
    "#  linreg_error[...] = ... \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1589030664c5f1d21df7ecf09d69cc6",
     "grade": true,
     "grade_id": "cell-b0017e97370ec75b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform some sanity checks on the result\n",
    "assert linreg_error.shape == (max_r,), \"'linreg_error' has the wrong shape.\"\n",
    "assert linreg_error[9] < 0.8 * linreg_error[2], \"training errors are not correct\"\n",
    "assert linreg_error[5] > linreg_error[6], \"training errors are not correct\"\n",
    "\n",
    "print('Sanity check passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66b6a8063e19529d20545c58365e23f6",
     "grade": false,
     "grade_id": "cell-dda261b03a29c37d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we print and plot the training errors in order to assess how well the different models fit the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc1ae94ade4c85b81c1e50cf634e5f3d",
     "grade": false,
     "grade_id": "cell-07988e3699bbe74b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the training errors\n",
    "print(f\"Training errors (rounded to 2 decimals): \\n {np.round(linreg_error, 2)}\")\n",
    "\n",
    "# create a numpy array \"r_values\" containing the values 1,2...,max_r \n",
    "r_values = np.linspace(1, max_r, max_r, endpoint=True)\n",
    "# create a plot object which can be accessed using variables \"fig\" and \"axes\"\n",
    "fig, axes = plt.subplots(1,1, figsize=(8, 5))\n",
    "# add a curve representing the average squared error for each choice of r \n",
    "axes.plot(r_values, linreg_error, label='MSE', color='red')\n",
    "# add captions for the horizontal and vertical axes \n",
    "axes.set_xlabel('features')\n",
    "axes.set_ylabel('empirical error')\n",
    "# add a title to the plot \n",
    "axes.set_title('Training error vs number of features')\n",
    "axes.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2527f86b33adc17bc2a52ed1c8279232",
     "grade": false,
     "grade_id": "cell-797d45ab06df1d04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you have completed the task correctly, you should see plot similar to this one:\n",
    "\n",
    "<img src=\"../../../coursedata/2_Regression/numoffeatures.png\" width=600/>\n",
    "\n",
    "As you can see, the training error is decreasing with respect to the number of features in the linear regression model. Based on this, it would seem that including more experimental features results in learning better predictor functions. In particular, using all 10 features of data points results in the smallest training error. \n",
    "\n",
    "It is tempting to conclude that we should use all available features of neighborhoods to predict their median apartment price. However, keep in mind that the actual purpose of the model is to predict the median apartment value of neighborhoods that are **not** in the training set $\\mathcal{D}$. As such, a low error on the training set is not necessarily a reliable indicator for a good performance on datapoints (neighborhoods) outside the training set $\\mathcal{D}$. We will see in notebook 3 of our course that if a predictor function incurs a small (even close to zero) error on a training set, it does not necessarily imply accurate predictions on data points outside the training set. \n",
    "\n",
    "Your next task is to explore the effect of using a different number of data points for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "989a9d7c553aa07647ebcae7c52a7a45",
     "grade": false,
     "grade_id": "cell-ab5cf2b4d9840b3b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "### Student Task 2.2. Varying Number of Data Points.\n",
    "    \n",
    "In this task, your objective is to train 10 different models that use the $m$ first data points of the dataset, with $m$ ranging from 1 to 10. That is, you should first train a model on the data using only the data point $\\mathbf{x}^{(1)}$, then a model using the data points $\\mathbf{x}^{(1)},\\mathbf{x}^{(2)}$, and so forth, up until the model that uses the 10 first data points $\\mathbf{x}^{(1)},\\mathbf{x}^{(2)},\\ldots,\\mathbf{x}^{(10)}$. You should also calculate and store the training error for each model in an array `train_error`.\n",
    "    \n",
    "The solution to this exercise is very similar to the one above, with the important difference that you are now varying the number of data points that you use to train the model on each iteration. This means that you must modify the feature matrix in a different manner to achieve the correct result.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0582ed29de79900a78bce37aeb229ad4",
     "grade": false,
     "grade_id": "cell-e57b15d397a6b503",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# maximum number of data points\n",
    "max_m = 10     \n",
    "\n",
    "# Load the dataset using n=2 features \n",
    "X, y = load_housing_data(n=2)   \n",
    "\n",
    "# Array in which to store the training errors of the different number of data points\n",
    "train_error = np.zeros(max_m)         \n",
    "\n",
    "### STUDENT TASK ###\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fb2ef60f948d46663f3a9f19c101339",
     "grade": true,
     "grade_id": "cell-d19e618cf3f2a6e6",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform sanity checks on the results\n",
    "assert train_error.shape == (10,), \"'train_error' has wrong dimensions.\"\n",
    "assert train_error[0] < 100 * train_error[3], \"training errors are not correct\"\n",
    "assert train_error[2] > train_error[1], \"training errors are not correct\"\n",
    "\n",
    "print('Sanity checks passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2d1abe65c16a917c20807e4c69ebb02",
     "grade": false,
     "grade_id": "cell-33585b6d3f2b306a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we print and plot the training errors in order to assess how well the different models fit the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5606d261665ce3eebfb45822bc9bc540",
     "grade": false,
     "grade_id": "cell-13913ca8963924a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Print training errors\n",
    "print(f\"Training Errors (Rounded to 2 decimals): \\n {np.round(train_error, 2)}\")\n",
    "\n",
    "# create a numpy array \"m_values\" containing the values 1,2...,max_m\n",
    "m_values = np.arange(1, max_m + 1)\n",
    "# create a plot object which can be accessed using variables \"fig\" and \"axes\"\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8, 4))\n",
    "# add a curve representing the average squared error for each choice of m\n",
    "axes.plot(m_values, train_error, label='MSE', color='red')\n",
    "# add captions for axes of the plot \n",
    "axes.set_xlabel('number of data points (sample size)')\n",
    "axes.set_ylabel('training error')\n",
    "# add title for the plot \n",
    "axes.set_title('Training error vs. number of data points')\n",
    "axes.legend()\n",
    "plt.tight_layout()\n",
    "# display the plot \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd416d800eb6baeb411629991e0a8afa",
     "grade": false,
     "grade_id": "cell-3d10953087fadc50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you have completed the task correctly, you should see plot similar to this one:\n",
    "\n",
    "<img src=\"../../../coursedata/2_Regression/numofdatapoints.png\" width=650/>\n",
    "\n",
    "\n",
    "As you can see, using more data points to fit a linear predictor results in an increasing training error. Why do you think increasing sample size leads to an increase in the training error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "112cb626471164d4f13d6080bd8eac91",
     "grade": false,
     "grade_id": "cell-ba2b95035f72f865",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## (Non-) Robustness Against Outliers\n",
    "\n",
    "Suppose that the domain experts work very long hours, and are often tired when entering apartment evaluation data into the dataset. Sometimes the tiredness causes careless mistakes that result in abnormally high or low values for the median apartment value of some neighborhood. The resulting data points differ significantly from the intact data points and are called **outliers**.\n",
    "\n",
    "In general we would like to have ML methods that are robust against outliers. Having some datapoints in the training set that are corrupted should not significantly affect the learnt predictor function (see ${\\rm (Eq3)}$). It turns out that using the squared error loss to learn a linear predictor yields a ML method that is quite sensitive to outliers. Intuitively, having an outlier results in a significantly different predictor function since otherwise the loss for the outlier would be too large. This is due to using the square $(y-\\hat{y})^2$ of the prediction error $(y-\\hat{y})$ which \"amplifies\" the effect of an outlier in the training data. We will see below that replacing the squared error loss with another loss function to learn a linear predictor provides more robustness against outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fa4e0925a62833d78e488828dcb539d",
     "grade": false,
     "grade_id": "cell-22eeb080ace826ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='drawplot'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "### Demo. Loading corrupted data.\n",
    "\n",
    "The code below provides a function that loads a single feature $x_1$ and the label $y$ for all datapoints (neighborhood) in our dataset $\\mathcal{D}$. The resulting features and labels are stored in numpy arrays `X` and `y`. We then corrupt one datapoint in $\\mathcal{D}$ by replacing the true label value with the random value $80$. The features and labels for the resulting corrupted dataset $\\mathcal{D}'$ are stored in the numpy arrays `X_cor` and `y_cor`. Both datasets are then visualized by separate scatterplots. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2983a766dedea1e6a4f15445be5de6f",
     "grade": false,
     "grade_id": "cell-6740feea5fe8118f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_corrupted_data():\n",
    "    X, y = load_housing_data(n=1)\n",
    "    \n",
    "    # perturb the label of the data point with lowest x_1\n",
    "    y[np.argmin(X)] = 80\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = load_housing_data(n=1)\n",
    "X_cor, y_cor = load_corrupted_data()\n",
    "\n",
    "# Plot the real and the corrupted datasets for comparison\n",
    "\n",
    "fig, ax = plt.subplots(1, 2,  figsize=(13,6))   # create a figure with two horizontal subplots\n",
    "ax[0].scatter(X, y)\n",
    "ax[0].set_xlabel('$x_1$: Average number of rooms')\n",
    "ax[0].set_ylabel(\"$y$: Median apartment value (10000€)\")\n",
    "ax[0].set_title(\"real-estate dataset $\\mathcal{D}$\")\n",
    "ax[1].scatter(X_cor, y_cor)\n",
    "ax[1].set_xlabel('$x_1$: Average number of rooms')\n",
    "ax[1].set_ylabel(\"$y$: Median apartment value (10000€)\")\n",
    "ax[1].set_title(\"corrupted dataset $\\mathcal{D}'$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db6b025298c759c76a3b54c54df9cc86",
     "grade": false,
     "grade_id": "cell-95e180ce6b62fbb2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='drawplot'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "### Demo. (Non-)Robustness of Linear Regression with Squared Error Loss \n",
    "    \n",
    "The following code learns two linear predictors using linear regression applied to two datasets $\\mathcal{D}$ and $\\mathcal{D}'$. The dataset $\\mathcal{D}'$ is obtained from $\\mathcal{D}$ by corrupting the label of a single datapoint in $\\mathcal{D}$. We illustrate the learnt predictor maps along with the datasets in two plots. Consider how the erroneous data point in the corrupted data has a significant effect on the optimal predictor function that minimizes the MSE.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70cc9a2bbc558e70808b3ff41d539d39",
     "grade": false,
     "grade_id": "cell-5f9aaa714f3351bf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# learn a linear predictor map by minimizing MSE incurred on the dataset D\n",
    "reg = LinearRegression(fit_intercept=True) \n",
    "reg = reg.fit(X, y)                       \n",
    "y_pred = reg.predict(X)                   \n",
    "\n",
    "# learn a linear predictor map by minimizing MSE incurred on corrupdated dataset D'\n",
    "reg_cor = LinearRegression(fit_intercept=True)\n",
    "reg_cor = reg_cor.fit(X_cor, y_cor)   \n",
    "y_pred_cor = reg_cor.predict(X_cor)   \n",
    "\n",
    "# Create a plot object \n",
    "fig, axes = plt.subplots(2, 1, sharex=True, figsize=(8, 8))   # create a figure with two vertical subplots \n",
    "\n",
    "# Plot a subplot 1 with original data\n",
    "axes[0].scatter(X, y, label='data')  # Plot data points\n",
    "axes[0].plot(X, y_pred, color='green', label='linear predictor')  # Plot linear predictor\n",
    "\n",
    "# For each data point, add line indicating prediction error\n",
    "axes[0].plot((X[0], X[0]), (y[0], y_pred[0]), color='red', label='errors')  # Add label to legend\n",
    "for i in range(len(X)-1):\n",
    "    lineXdata = (X[i+1], X[i+1])  # Same X\n",
    "    lineYdata = (y[i+1], y_pred[i+1])  # Different Y\n",
    "    axes[0].plot(lineXdata, lineYdata, color='red')\n",
    "\n",
    "# Set axes title, labels and legend\n",
    "axes[0].set_title('linear regression on dataset $\\mathcal{D}$')\n",
    "axes[0].set_ylabel(\"$y$: Median apartment value (10000€)\")\n",
    "axes[0].set_ylim(10,40)  # set y-axis range to 0 till 100\n",
    "axes[0].legend()\n",
    "\n",
    "\n",
    "# Plot a subplot 2 with corrupted data\n",
    "axes[1].scatter(X_cor, y_cor, label='data') \n",
    "axes[1].set_ylim(10,40)  \n",
    "axes[1].plot(X, y_pred_cor, color='green')  \n",
    "\n",
    "# plot prediction errors\n",
    "for i in range(len(X)):\n",
    "    lineXdata = (X[i], X[i])  \n",
    "    lineYdata = (y_cor[i], y_pred_cor[i]) \n",
    "    axes[1].plot(lineXdata, lineYdata, color='red')\n",
    "\n",
    "# set axes title, labels and legend\n",
    "axes[1].set_title('linear regression on perturbed dataset $\\mathcal{D}{\\'}$')\n",
    "axes[1].set_xlabel('$x_1$: Average number of rooms')\n",
    "axes[1].set_ylabel(\"$y$: Median apartment value (10000€)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"optimal weight w_opt by fitting to (training on) clean training data : \", reg.coef_[0])\n",
    "print(\"optimal weight w_opt by fitting to (training on) corrupted training data : \", reg_cor.coef_[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "894967b8ebe9bedc1ca0d75475055229",
     "grade": false,
     "grade_id": "cell-45ac1f059329f66f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The figure above shows that the erroneous data point significantly affects the optimal predictor function obtained by minimizing the MSE on $\\mathcal{D}'$. in other words, the linear predictor learnt by minimizing the MSE on the dataset $\\mathcal{D}'$ depends heavily on a single outlier. We can therefore expect this predictor function to incur a large prediction error for datapoints outside $\\mathcal{D}'$. A simple yet powerful approach to obtain more robustness against outliers is to replace the squared error loss with a different loss function which we introduce next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45192bd8e68e615c843e8d77be3adf58",
     "grade": false,
     "grade_id": "cell-2841c0fccc7c99a9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## The Huber Loss\n",
    "\n",
    "Learning a linear predictor function by minimizing the squared error loss $(h(\\mathbf{x}) - y)^{2}$ forces learnt predictor $h(\\mathbf{x})$ to not be too far away from any data point in the training set including the outlier. This, in turn, means that if we replace a single data point by an outlier far away, then also the predictor function must change substantially to keep the squared error small. \n",
    " \n",
    "It turns out that using a different loss function to learn a linear predictor can make the learning robust against few outliers. One such loss function is known as the [\"Huber loss\"](https://en.wikipedia.org/wiki/Huber_loss) $\\mathcal{L}(\\hat{y},y)$, and is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(y,\\hat{y}) = \\begin{cases} (1/2) (y-\\hat{y})^{2} & \\mbox{ for } |y-\\hat{y}| \\leq   \\varepsilon \\\\ \n",
    "\\varepsilon(|y-\\hat{y}| - \\varepsilon/2) & \\mbox{ else. }\\end{cases} \n",
    "\\tag{Eq4}\n",
    "\\end{equation}\n",
    "\n",
    "**The Huber loss is robust to outliers since the errors for which $|y - \\hat{y}| > \\varepsilon$ are not squared**. Thus, these data points have a smaller effect on the total loss over the dataset and hence the resulting fit. Note that the Huber loss contains a parameter $\\varepsilon$, which has to be adapted to the application at hand. A principled approach to choose the value of $\\varepsilon$ is to try out several candidate values and then pick the one resulting in the best overall performance. This overall performance of a ML method can be measured by one of the validation techniques discussed in notebook 3 of this course. \n",
    "\n",
    "The Huber loss ${\\rm (Eq4)}$ includes two important special cases. The first special case is when $\\varepsilon$ is chosen very large (the precise value depending on the value range of the features and labels), such that the condition $|y-\\hat{y}| \\leq \\varepsilon$ is always satisfied. In this case, the Huber loss is equivalent to the squared error loss $(y-\\hat{y})^{2}$ (with an additional factor 1/2). \n",
    "\n",
    "Another special case of the Huber loss ${\\rm (Eq4)}$ is obtained when $\\varepsilon$ is very small (close to $0$) such that the condition $|y-\\hat{y}| \\leq \\varepsilon$ is never satisfied. In this case, the Huber loss reduces to the absolute loss $|y - \\hat{y}|$ (scaled by a factor $\\varepsilon$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "747f55bdaa3b3ead0467f409a666604e",
     "grade": false,
     "grade_id": "cell-616d45f35afde733",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"../../../coursedata/2_Regression/huber1.png\" width=650/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da791b7c0dad6917f9753e3cf2b5dc8a",
     "grade": false,
     "grade_id": "cell-cf09b243fb1ad685",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='drawplot'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "\n",
    "### Demo. Squared Error and Huber Loss\n",
    "<p>\n",
    "The code below plots the squared error loss and the Huber loss for different choices of the parameter $\\varepsilon$. Note that the Huber loss reduces to the squared error loss for a sufficiently large value of the parameter $\\varepsilon$.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b88c9a8434027ec4eef1a381cfee20f1",
     "grade": false,
     "grade_id": "cell-5dfd931359725f65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the Huber loss\n",
    "def HuberLoss(pred_error, epsilon):\n",
    "    # pred_error - prediction error y-y_pred\n",
    "    # epsilon - parameter epsilon 𝜀 \n",
    "    pred_error = abs(pred_error)\n",
    "    flag = (pred_error > epsilon)\n",
    "    return (~flag) * (0.5 * pred_error ** 2) - (flag) * epsilon * (0.5 * epsilon - pred_error)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot for several values of epsilon\n",
    "fig = plt.figure(figsize=(10, 5)) # set figure size\n",
    "ax = fig.add_subplot(111) # add 1 subplot\n",
    "\n",
    "pred_error = np.linspace(-5, 5, 100) # create linear space from -5 to 5 with 100 steps\n",
    "\n",
    "for epsilon in (1, 2, 10): # loop through values 1, 2, 10\n",
    "    loss = HuberLoss(pred_error, epsilon)\n",
    "    ax.plot(pred_error, loss, '-k') # plot x and y\n",
    "\n",
    "    if epsilon > 10:\n",
    "        s = r'\\infty' # set s to infinity sign (string format)\n",
    "    else:\n",
    "        s = str(epsilon) # set s to string of number epsilon\n",
    "\n",
    "    ax.text(pred_error[6], loss[6], '$\\epsilon=%s$' % s,\n",
    "            ha='center', va='center',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w')) # add test to each line\n",
    "\n",
    "ax.plot(pred_error, np.square(pred_error),label=\"squared loss\") # plot the sqared loss (blue line)\n",
    "\n",
    "ax.set_xlabel(r'Error: $y - \\hat{y}$') # set x labels\n",
    "ax.set_ylabel(r'loss $\\mathcal{L}(y,\\hat{y})$') # set y label\n",
    "ax.legend() # show legend in plot\n",
    "plt.show() # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d899ad2b88fde279659ecbbe882fce83",
     "grade": false,
     "grade_id": "cell-87c2d62347509737",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='drawplot'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student task 2.3. Robustness of Linear Regression with Huber Loss.\n",
    "\n",
    "In this task, your objective is to train separate Huber regression model on the original and corrupted data and use these models to calculate the predicted labels on the datasets. You should store the predicted labels for the model using the original data in the variable `y_pred`, and the predicted labels for the corrupted data in the variable `y_pred_cor`.\n",
    "    \n",
    "A Huber Regression model is implemented in the Scikit-learn class `HuberRegressor`. The methods of the `HuberRegressor` class are constructed similarly to the `LinearRegression` class, so training the models and calculating the predicted labels is done using the same function names `.fit()` and `.predict()`.\n",
    "    \n",
    "Use the default $\\varepsilon$ value ($\\varepsilon$=1.35) defined in [`HuberRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn-linear-model-huberregressor) class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d042e4b521ce19d3303963917633b082",
     "grade": false,
     "grade_id": "cell-53fc0a75788a46a8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "X, y = load_housing_data(n=1)\n",
    "X_cor, y_cor = load_corrupted_data()   # read in 20 data points with single feature x_1 and label y \n",
    "\n",
    "### STUDENT TASK ###\n",
    "# Huber regression model on the original dataset\n",
    "# reg = ...\n",
    "# reg...\n",
    "# y_pred = ...\n",
    "#\n",
    "# Huber regression model on the corrupted dataset\n",
    "# reg_cor = ...\n",
    "# reg_cor...\n",
    "# y_pred_cor = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"optimal weight w_opt by fitting on clean data : \", reg.coef_[0])\n",
    "print(\"optimal weight w_opt by fitting on perturbed data : \", reg_cor.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36bb46ca37f82b11c428198c3a20c765",
     "grade": true,
     "grade_id": "cell-6f4ce8e672d7c9ea",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform some sanity checks on the output\n",
    "assert y_pred.shape == (20,), \"The shape of y_pred is wrong!\"\n",
    "assert y_pred_cor.shape == (20,), \"The shape of y_pred_cor is wrong!\"\n",
    "\n",
    "print(\"Sanity checks passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0eec9524eed4e367bf5e8259f4c8727b",
     "grade": true,
     "grade_id": "cell-e303927c921db723",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "882fddf44d351cce1a8001053497e8c0",
     "grade": false,
     "grade_id": "cell-8e7e1eab76b4b1ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a plot object \n",
    "fig, axes = plt.subplots(2, 1, sharex=True, figsize=(8, 8))   # create a figure with two vertical subplots \n",
    "\n",
    "# Plot a subplot 1 with original data\n",
    "axes[0].scatter(X, y, label='data')  # Plot data points\n",
    "axes[0].plot(X, y_pred, color='green', label='linear predictor')  # Plot linear predictor\n",
    "\n",
    "# For each data point, add line indicating prediction error\n",
    "axes[0].plot((X[0], X[0]), (y[0], y_pred[0]), color='red', label='errors')  # Add label to legend\n",
    "for i in range(len(X)-1):\n",
    "    lineXdata = (X[i+1], X[i+1])  # Same X\n",
    "    lineYdata = (y[i+1], y_pred[i+1])  # Different Y\n",
    "    axes[0].plot(lineXdata, lineYdata, color='red')\n",
    "\n",
    "# Set axes title, labels and legend\n",
    "axes[0].set_title('linear regression on dataset $\\mathcal{D}$')\n",
    "axes[0].set_ylabel(\"$y$: Median apartment value (10000€)\")\n",
    "axes[0].set_ylim(10,40)  # set y-axis range to 0 till 100\n",
    "axes[0].legend()\n",
    "\n",
    "\n",
    "# Plot a subplot 2 with corrupted data\n",
    "axes[1].scatter(X_cor, y_cor, label='data') \n",
    "axes[1].set_ylim(10,40)  \n",
    "axes[1].plot(X, y_pred_cor, color='green')  \n",
    "\n",
    "# plot prediction errors\n",
    "for i in range(len(X)):\n",
    "    lineXdata = (X[i], X[i])  \n",
    "    lineYdata = (y_cor[i], y_pred_cor[i]) \n",
    "    axes[1].plot(lineXdata, lineYdata, color='red')\n",
    "\n",
    "# set axes title, labels and legend\n",
    "axes[1].set_title('linear regression on perturbed dataset $\\mathcal{D}{\\'}$')\n",
    "axes[1].set_xlabel('$x_1$: Average number of rooms')\n",
    "axes[1].set_ylabel(\"$y$: Median apartment value (10000€)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"optimal weight w_opt by fitting to (training on) clean training data : \", reg.coef_[0])\n",
    "print(\"optimal weight w_opt by fitting to (training on) corrupted training data : \", reg_cor.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec63e8c944a083f609fd637b4f9d6d1b",
     "grade": false,
     "grade_id": "cell-c77f887d9194b713",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can see that the predictors trained on the original and perturbed data are very similar when using the Huber loss, in contrast to the large difference in the predictors that minimize the mean-squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "148453b508691e83ccdc3d7d35851508",
     "grade": false,
     "grade_id": "cell-1407fa4ba7cde634",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Key takeaways\n",
    "\n",
    "- **Regression problems** amounts to predicting the numeric label $y$ of a data point based on its features $\\mathbf{x}$. \n",
    "\n",
    "\n",
    "\n",
    "- A widely used hypothesis space for regression methods is the space of linear functions $\\hat{y} = h(\\mathbf{x}) = w_{0}+w_1x_1 + w_2x_2 + \\ldots + w_nx_n$. \n",
    "\n",
    "\n",
    "- Regression methods using linear functions work well if there is an (approximately) linear relation between features and label of datapoints.\n",
    "\n",
    "- A large class of regression methods uses the squared error loss $\\mathcal{L}(y,\\hat{y}) = (y-\\hat{y})^{2}$. These methods learn a predictor function $h(\\mathbf{x})$ by minimizing the average (mean) squared error (MSE)\n",
    "\\begin{equation}\n",
    "   \\frac{1}{m} \\sum_{i=1}^m (y^{(i)} - \\hat{y}^{(i)})^2 =  \\frac{1}{m} \\sum_{i=1}^m \\big(y^{(i)} - h\\big(\\mathbf{x}^{(i)}\\big)\\big)^2, \n",
    "\\end{equation}\n",
    "on a labeled dataset. \n",
    "\n",
    "- The minimum MSE achieved by the optimal linear predictor is referred as the training error of a linear regression method. The training error depends on the number of features used to characterize datapoints as well as on the number of datapoints used for the training set. \n",
    "\n",
    "- Another popular choice for the loss function in regression problems is the Huber loss,\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(y,\\hat{y}) = \\begin{cases} (1/2) (y-\\hat{y})^{2} & \\mbox{ for } |y-\\hat{y}| \\leq   \\varepsilon \\\\ \n",
    "\\varepsilon(|y-\\hat{y}| - \\varepsilon/2) & \\mbox{ else. }\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "- ML methods that use the Huber loss instead of the squared error loss tend to be more robust against the presence of few outliers in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9c71a23a97210233b5a530e5f7c3a82",
     "grade": false,
     "grade_id": "cell-7be3ef3c099fc592",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Quiz Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40ad51f4e1821c5b6e079448a7313beb",
     "grade": false,
     "grade_id": "cell-f8224d30fbc9593e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR2_1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <p><b>Student Task.</b> Question 2.1. </p>\n",
    "\n",
    " <p>When is a machine learning problem called a regression problem?</p>\n",
    "\n",
    "<ol>\n",
    "  <li> When the quantity of interest (the label) is a numeric quantity. </li>\n",
    "  <li> When the data is stored in a spreadsheet.</li>\n",
    "  <li> When the method uses audio data.  </li>\n",
    "  <li> When the quantity of interest takes on only a finite number of different values (e.g. \"-1\",\"0\" or \"4\").\n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43b39ee72e3b7f194b67e3c3c74166e8",
     "grade": false,
     "grade_id": "cell-9b4f59d7c4d6726d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### STUDENT TASK ###\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "# answer_Q1  = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86e22cced8d2fa0df57029e5986a9c86",
     "grade": true,
     "grade_id": "cell-04494bd6dec74741",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "\n",
    "assert answer_Q1 in [1,2,3,4], '\"answer_Q1\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46be5e62d2077824d525be8daa437ee6",
     "grade": false,
     "grade_id": "cell-c0de5e8e503aeac1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR2_2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <p><b>Student Task.</b> Question 2.2.</p>\n",
    "    <p> What is the effect of using more features for learning (fitting) a linear predictor by minimizing the average squared error on training data?</p>\n",
    "    <ol>\n",
    "      <li> The training error increases. </li>\n",
    "      <li> The training error decreases. </li>\n",
    "      <li> The training error does not depend on the number of features. </li>\n",
    "    </ol> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2981c0c57806b2837d2d141be281e1a1",
     "grade": false,
     "grade_id": "cell-a43c10bae7ac8878",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### STUDENT TASK ###\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "# answer_Q2  = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "022e91ddfddf44db5f2d08b36ef8ed1d",
     "grade": true,
     "grade_id": "cell-5510099c69e99cbd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "\n",
    "assert answer_Q2 in [1,2,3], '\"answer_Q2\" Value should be an integer between 1 and 3.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47f82594480f7a004b4ff273fa352eba",
     "grade": false,
     "grade_id": "cell-35ca27f13a2a7e20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR2_3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <p><b>Student Task.</b> Question 2.3.</p>\n",
    "    <p> How does the resulting regression method differ when using either squared error or Huber loss?</p>\n",
    "    <ol>\n",
    "      <li> Using Huber loss makes the resulting method more robust against outliers, i.e., the learned predictor does not vary too much if the dataset includes a few outliers.  </li>\n",
    "      <li> Using squared error loss makes the resulting method more robust against outliers.  </li>\n",
    "    </ol> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b19479affabb996e1edf7ed4e9da8a47",
     "grade": false,
     "grade_id": "cell-e207a14dd4e3fbb2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### STUDENT TASK ###\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "# answer_Q3  = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e965cf31595b772b959934c0e04b7ae9",
     "grade": true,
     "grade_id": "cell-d91289ea734887d4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "\n",
    "assert answer_Q3 in [1,2], '\"answer_Q3\" Value should be an integer between 1 and 2.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d01f6e5e428f62c02c082421c85790a",
     "grade": false,
     "grade_id": "cell-9746fa0be6499cd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR2_4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <p><b>Student Task.</b> Question 2.4.</p>\n",
    "    <p> For a fixed dataset, will regression models fitted with Huber loss and squared error loss always differ from one another?</p>\n",
    "    <ol>\n",
    "      <li> Yes, because in contrast to square loss, Huber loss for $|y-\\hat{y}| \\leq   \\varepsilon$ has an additional constant factor 1/2: $\\frac{1}{2}(y-\\hat{y})^2$.</li>\n",
    "      <li> No, they produce essentially the same results. </li>\n",
    "      <li> No, not always. If there are no outliers (measured/quantified with respect to the parameter epsilon $\\varepsilon$) Huber loss behaves like a squared error loss. </li>\n",
    "    </ol> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe5291e6c88b9df3b319e658048454b0",
     "grade": false,
     "grade_id": "cell-b635e95c030ab8a0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### STUDENT TASK ###\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "# answer_Q4  = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "647fdf4bdb701a44c1dcc2bae4e8d222",
     "grade": true,
     "grade_id": "cell-8919eb7bdf2e8e61",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "\n",
    "assert answer_Q4 in [1,2,3], '\"answer_Q4\" Value should be an integer between 1 and 3.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287.5735168457031px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
